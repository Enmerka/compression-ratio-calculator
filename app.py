# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UkcIO5NUPwdr8I02_AL_5XCvPx33B4cZ
"""

import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import gzip

# Function to fetch and parse a webpage
def fetch_and_parse(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
    except requests.RequestException as e:
        st.error(f"Error fetching URL {url}: {e}")
        return None

    for tag in soup(['head', 'header', 'footer', 'script', 'style', 'meta']):
        tag.decompose()
    return soup

# Function to extract and combine text from the page
def extract_text_selectively(soup):
    if not soup:
        return ""

    text_lines = []
    for element in soup.find_all(['p', 'li', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'table', 'tr'], recursive=True):
        inline_text = ' '.join(element.stripped_strings)
        if inline_text:
            text_lines.append(inline_text)

    return ' '.join(text_lines)

# Function to calculate compression ratio
def calculate_compression_ratio(text):
    if not text:
        return 0
    original_size = len(text.encode('utf-8'))
    compressed_data = gzip.compress(text.encode('utf-8'))
    compressed_size = len(compressed_data)
    return original_size / compressed_size if compressed_size else 0

# Streamlit App
st.title("URL Compression Ratio Calculator")

# File uploader
uploaded_file = st.file_uploader("Upload an Excel file with a column named 'URL'", type=["xlsx"])

if uploaded_file:
    df = pd.read_excel(uploaded_file)
    st.write("Uploaded Data Preview:", df.head())

    if 'URL' in df.columns:
        compression_ratios = []

        # Process each URL
        for index, row in df.iterrows():
            url = row['URL']
            st.write(f"Processing URL: {url}")
            
            soup = fetch_and_parse(url)
            combined_text = extract_t
